{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxgolOTnlT4Q"
      },
      "source": [
        "#Enhancher type prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ddIB9cIk0kk"
      },
      "source": [
        "##Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gZFC7z1kfRP",
        "outputId": "88d13a76-6169-4a57-d8d6-c7f37a8d3d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import nucleotide_transformer\n",
        "except:\n",
        "    !pip install numpy==1.23.5\n",
        "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
        "    import nucleotide_transformer\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    from jax.tools import colab_tpu\n",
        "\n",
        "    colab_tpu.setup_tpu()\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from nucleotide_transformer.pretrained import get_pretrained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FXpse8YVm3Gp"
      },
      "outputs": [],
      "source": [
        "#@title Select a model\n",
        "#@markdown ---\n",
        "model_name = '500M_human_ref'#@param['500M_human_ref', '500M_1000G', '2B5_1000G', '2B5_multi_species', '50M_multi_species_v2', '100M_multi_species_v2', '250M_multi_species_v2', '500M_multi_species_v2']\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4LabXa92nUW0"
      },
      "outputs": [],
      "source": [
        "# Get pretrained model\n",
        "embedding_layer = 20  # Select layer to extract embeddings from\n",
        "\n",
        "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
        "    model_name=model_name,\n",
        "    embeddings_layers_to_save=(embedding_layer,),\n",
        "    max_positions=250\n",
        ")\n",
        "forward_fn = hk.transform(forward_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVrTy8zfkwBS"
      },
      "source": [
        "##Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AFNalEM4ldxI"
      },
      "outputs": [],
      "source": [
        "# Install\n",
        "!pip install -q biopython transformers datasets huggingface_hub accelerate\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "dataset_name = \"enhancers\"\n",
        "train_dataset = load_dataset(\n",
        "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
        "        dataset_name,\n",
        "        split=\"train\",\n",
        "        streaming= False,\n",
        "    )\n",
        "\n",
        "test_dataset = load_dataset(\n",
        "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
        "        dataset_name,\n",
        "        split=\"test\",\n",
        "        streaming= False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HefsaszHip79"
      },
      "source": [
        "## Split features from label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "v3niosDvixvt"
      },
      "outputs": [],
      "source": [
        "# Training data\n",
        "train_sequences = train_dataset['sequence']\n",
        "train_labels = train_dataset['label']\n",
        "\n",
        "# Test data\n",
        "test_sequences = test_dataset['sequence']\n",
        "test_labels = test_dataset['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riakuSpqr5T5"
      },
      "source": [
        "##Retrieve embeddings (one for every sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iaG9ivubHEXi"
      },
      "outputs": [],
      "source": [
        "batch_size = 8  # Adjust to available memory\n",
        "\n",
        "def get_seq_embeddings(sequences: list, batch_size: int):\n",
        "    embeddings = []  # Stores 6mers embeddings\n",
        "\n",
        "    # Split sequences into batches\n",
        "    batched_sequences = [sequences[i:i + batch_size] for i in range(0, len(sequences), batch_size)]\n",
        "\n",
        "    random_key = jax.random.PRNGKey(0)\n",
        "    extraction_layer = \"embeddings_\" + str(embedding_layer)\n",
        "    cls_token_position = 0 # Position of the CLS token for every sequence\n",
        "\n",
        "    for batch in batched_sequences:\n",
        "        # Tokenize the batch\n",
        "        tokens_ids = [b[1] for b in tokenizer.batch_tokenize(batch)]\n",
        "        tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
        "\n",
        "        # Infer\n",
        "        outs = forward_fn.apply(parameters, random_key, tokens)\n",
        "\n",
        "        # Get embeddings\n",
        "        batch_embeddings = outs[extraction_layer]\n",
        "\n",
        "        # Mask for padding tokens and CLS token\n",
        "        padding_mask = (tokens != tokenizer.pad_token_id) & (jnp.arange(tokens.shape[1]) != cls_token_position)\n",
        "        masked_embeddings = batch_embeddings * padding_mask[:, :, None]\n",
        "\n",
        "        for item in masked_embeddings:\n",
        "            sum_embeddings = jnp.sum(item, axis=-1)\n",
        "            non_zero_mask = sum_embeddings != 0.0\n",
        "            seq_token_embeddings = item[non_zero_mask]\n",
        "            sequences_lengths = item.shape[0]\n",
        "            mean_embedding = jnp.sum(seq_token_embeddings, axis=0) / sequences_lengths\n",
        "            embeddings.append(mean_embedding)\n",
        "\n",
        "    return jnp.vstack(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = get_seq_embeddings(train_sequences, batch_size)\n",
        "X_test = get_seq_embeddings(test_sequences, batch_size)\n",
        "\n",
        "y_train = train_labels\n",
        "y_test = test_labels"
      ],
      "metadata": {
        "id": "WbjEtBQo6_nH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwXCRIBcmjK-"
      },
      "source": [
        "##Train downstream model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqx3e-nr3YNY",
        "outputId": "d06b0b4f-fd5e-40d3-f4b9-9f9a81512ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.11.4)\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Info] Number of positive: 7484, number of negative: 7484\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.211321 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 326400\n",
            "[LightGBM] [Info] Number of data points in the train set: 14968, number of used features: 1280\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "Accuracy: 0.75\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_error',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "clf = lgb.LGBMClassifier(**params)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_iAof3msXW"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}